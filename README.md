# **ğŸ“Š Polynomial Regression Analysis with Bias-Variance Tradeoff**

## **ğŸ” Overview**
This project implements **polynomial regression** and explores:
- Cross-validation loss vs. model complexity
- Loss function implementations (MSE, MAE, Log loss, Linex loss)
- Bias-variance analysis with Bootstrap sampling
- Optimization of polynomial fitting using `scipy.optimize.minimize`
- Residual analysis after train-test split

ğŸ“Œ **Key Topics Covered:**
âœ” Polynomial Regression  
âœ” Loss Functions  
âœ” Cross-Validation  
âœ” Bias-Variance Tradeoff  
âœ” Optimization using `scipy.optimize.minimize`  
âœ” Residual Plots for Model Evaluation  

---

## **ğŸ“‚ Project Structure**
```
ğŸ“¦ Polynomial-Regression-Analysis 
 â”œâ”€â”€ ğŸ“‚ notebooks             # Jupyter Notebooks
 â”‚   â”œâ”€â”€ Polynomial_Regression_Analysis.ipynb  # Notebook containing solutions
 â”œâ”€â”€ README.md                # Documentation
 â”œâ”€â”€ requirements.txt         # Dependencies
 â”œâ”€â”€ .gitignore               # Ignore unnecessary files
```

---

## **ğŸ“¥ Installation**
To run this project, first **clone the repository**:
```sh
git clone https://github.com/SONIBARE/Polynomial-Regression-Analysis.git
cd Polynomial-Regression-Analysis
```

Install required dependencies:
```sh
pip install -r requirements.txt
```

Run Jupyter Notebook:
```sh
jupyter notebook
```

---

## **ğŸ“Š Tasks Implemented**
### **ğŸ”¹ Task 1: Cross-Validation Loss vs. Model Complexity**
- Uses k-fold cross-validation to estimate the loss for different polynomial degrees.
- Plots **degree vs. cross-validation loss**.
- **Identifies optimal complexity** using a vertical line.

### **ğŸ”¹ Task 2: Synthetic Dataset Generation**
- Implements polynomial evaluation function.
- Generates synthetic datasets with Gaussian noise.
- Plots **noisy data vs. true polynomial curve**.

### **ğŸ”¹ Task 3: Implementing Loss Functions**
âœ” **Mean Squared Error (MSE)**  
âœ” **Mean Absolute Error (MAE)**  
âœ” **Hinge Loss**  
âœ” **Log Loss (Binary Cross-Entropy)**  
âœ” **Linex Loss (Asymmetric Loss)**  

### **ğŸ”¹ Task 4: Polynomial Fitting using Optimization**
- Uses `scipy.optimize.minimize` to optimize polynomial coefficients.
- Implements a flexible **loss function-based fitting approach**.

### **ğŸ”¹ Task 5: Visualizing Residuals**
- Splits dataset into **train-test**.
- Fits a polynomial and **plots residuals**.
- Computes **Mean Squared Error on test set**.

### **ğŸ”¹ Task 6: Implementing a Polynomial Estimator Class**
- Object-oriented **PolynomialEstimator** class.
- Methods for **fit()** and **predict()**.

### **ğŸ”¹ Task 7: Bias-Variance Estimation via Bootstrap Sampling**
- Generates **bootstrap samples**.
- Computes **bias-variance decomposition**.
- Plots **bias vs. variance** for different models.

### **ğŸ”¹ Task 8: Comparing Underfitting vs. Overfitting Models**
- Uses **bootstrap sampling** to compare two models.
- Determines if **one model underfits and the other overfits**.

---

## **ğŸ“Š Sample Visualizations**
ğŸ“Œ **Cross-Validation Loss vs. Model Complexity**
![Cross-Validation Plot](plots/cv_loss_vs_degree.png)

ğŸ“Œ **Residual Analysis**
![Residuals](plots/residuals.png)

ğŸ“Œ **Bias-Variance Tradeoff**
![Bias-Variance](plots/bias_variance.png)

---



